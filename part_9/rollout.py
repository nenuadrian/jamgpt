from __future__ import annotations
import torch
from typing import List

from tokenizer_bpe import BPETokenizer


class RLHFTokenizer:
    def __init__(
        self, block_size: int, bpe_dir: str | None = None, vocab_size: int = 8000
    ):
        self.block_size = block_size
        self.tok = BPETokenizer(vocab_size=vocab_size)
        if bpe_dir:
            self.tok.load(bpe_dir)

    @property
    def vocab_size(self) -> int:
        return getattr(self.tok, "vocab_size", 256)

    def encode(self, text: str) -> List[int]:
        ids = self.tok.encode(text)
        if isinstance(ids, torch.Tensor):
            ids = ids.tolist()
        return ids

    def decode(self, ids: List[int]) -> str:
        if hasattr(self.tok, "decode"):
            return self.tok.decode(ids)
        return bytes(ids).decode("utf-8", errors="ignore")


def shift_labels(x: torch.Tensor) -> torch.Tensor:
    # For causal LM: predict x[t+1] from x[:t]
    return x[:, 1:].contiguous()


def gather_logprobs(logits: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:
    """Compute per-token logprobs of the given labels.
    logits: (B,T,V), labels: (B,T) over same T
    returns: (B,T) log p(labels)
    """
    logp = torch.log_softmax(logits, dim=-1)
    return logp.gather(-1, labels.unsqueeze(-1)).squeeze(-1)


@torch.no_grad()
def model_logprobs(model, x: torch.Tensor) -> torch.Tensor:
    # compute log p(x[t+1] | x[:t]) for t
    logits, _, _ = model.lm(x, None) if hasattr(model, "lm") else model(x, None)
    labels = shift_labels(x)
    lp = gather_logprobs(logits[:, :-1, :], labels)
    return lp  # (B, T-1)


# ---------- KL ----------


def approx_kl(policy_logp: torch.Tensor, ref_logp: torch.Tensor) -> torch.Tensor:
    # Mean over tokens: KL(pi||ref) â‰ˆ (logp_pi - logp_ref).mean()
    return (policy_logp - ref_logp).mean()


# ---------- small prompt source ----------
try:
    from datasets import load_dataset as _load_ds
except Exception:
    _load_ds = None


def sample_prompts(n: int) -> List[str]:
    if _load_ds is not None:
        try:
            ds = _load_ds("tatsu-lab/alpaca", split="train[:24]")
            arr = []
            for r in ds:
                inst = (r.get("instruction") or "").strip()
                inp = (r.get("input") or "").strip()
                if inp:
                    inst = inst + "\n" + inp
                if inst:
                    arr.append(inst)
                if len(arr) >= n:
                    break
            if arr:
                return arr
        except Exception:
            pass
    # fallback
    base = [
        "Explain the purpose of attention in transformers.",
        "Give two pros and cons of BPE tokenization.",
        "Summarize why PPO is used in RLHF.",
        "Write a tiny Python function that reverses a list.",
    ]
    return (base * ((n + len(base) - 1) // len(base)))[:n]
